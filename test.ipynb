{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\umbalaxibua\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Khai bao bien\n",
    "pdf_data_path = \"data\"\n",
    "vector_db_path = \"vectorstores/db_faiss\"\n",
    "\n",
    "def create_db_from_files():\n",
    "    # Load all PDFs from the folder\n",
    "    loader = DirectoryLoader(pdf_data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print('Done chunking')\n",
    "    # Set up Gemini API key\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"\"  # Thay bằng API key của bạn\n",
    "\n",
    "    # Use Gemini embedding model\n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "    # Create FAISS vector database (use from_documents, NOT from_texts)\n",
    "    db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "    # Save database\n",
    "    db.save_local(vector_db_path)\n",
    "    return db\n",
    "\n",
    "create_db_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Why There Is No Documentation Phase', 'result': 'Không có giai đoạn lập tài liệu riêng biệt vì việc lập tài liệu là một quá trình liên tục và song hành với toàn bộ quá trình phát triển phần mềm.  Việc thiếu tài liệu đầy đủ và chính xác sẽ dẫn đến các vấn đề nghiêm trọng như:  không thể kiểm tra xem phần mềm có hoạt động đúng hay không và gần như không thể bảo trì phần mềm.  Tóm lại, lập tài liệu không phải là một giai đoạn riêng biệt mà là một phần không thể thiếu xuyên suốt quá trình phát triển phần mềm.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Cấu hình API Key của Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"  # Thay bằng API key của bạn\n",
    "\n",
    "# Đường dẫn database vector\n",
    "vector_db_path = \"vectorstores/db_faiss\"\n",
    "\n",
    "# Load LLM từ API Gemini\n",
    "def load_llm():\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.01)\n",
    "    return llm\n",
    "\n",
    "# Tạo prompt template\n",
    "def create_prompt():\n",
    "    template = \"\"\"Bạn là một trợ lý AI thông minh, được sử dụng để trả lời câu hỏi dựa trên dữ liệu cung cấp.\n",
    "Dưới đây là ngữ cảnh từ tài liệu:\n",
    "{context}\n",
    "\n",
    "Câu hỏi: {question}\n",
    "\n",
    "Hãy trả lời một cách đầy đủ và dễ hiểu bằng tiếng việt dựa trên thông tin có trong ngữ cảnh. Nếu không có thông tin phù hợp, hãy nói rằng bạn không biết.\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt\n",
    "\n",
    "# Đọc dữ liệu từ VectorDB\n",
    "def read_vectors_db():\n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "    db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    return db\n",
    "\n",
    "# Tạo chain truy vấn\n",
    "def create_qa_chain(prompt, llm, db):\n",
    "    llm_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=False,\n",
    "        chain_type_kwargs={'prompt': prompt}\n",
    "    )\n",
    "    return llm_chain\n",
    "\n",
    "# Khởi tạo các thành phần\n",
    "db = read_vectors_db()\n",
    "llm = load_llm()\n",
    "prompt = create_prompt()\n",
    "llm_chain = create_qa_chain(prompt, llm, db)\n",
    "\n",
    "# Chạy chain với câu hỏi\n",
    "question = \"Why There Is No Documentation Phase\"\n",
    "response = llm_chain.invoke({\"query\": question})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
